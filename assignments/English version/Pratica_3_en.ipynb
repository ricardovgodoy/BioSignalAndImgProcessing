{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# Python practice 3: feature extraction and classification\n",
   "metadata": {
    "id": "be4bj8JNN9Zw"
   }
  },
  {
   "cell_type": "markdown",
   "source": "Let's start by importing the necessary libraries and loading/creating some example images that we will use throughout this practice.\n",
   "metadata": {
    "id": "SoSREcRMOBci"
   }
  },
  {
   "cell_type": "code",
   "source": "# --- Required Imports ---\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom skimage import data, img_as_float, img_as_ubyte, io, measure, morphology, feature, transform, color, filters\nfrom scipy import ndimage\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score\nimport cv2 # OpenCV for some operations such as template matching and moments\nimport os\nfrom numpy.fft import fft, ifft, fft2, ifft2, fftshift, ifftshift\n\n# --- Helper Plotting Function ---\ndef plot_many_images_c11_12(images, titles, rows=1, cols=None, cmaps=None, figsize=(15,10), main_title=None):\n    num_images = len(images)\n    if cols is None:\n        cols = (num_images + rows - 1) // rows\n\n    fig, axes = plt.subplots(rows, cols, figsize=figsize, squeeze=False)\n    axes_flat = axes.ravel()\n\n    if cmaps is None:\n        cmaps_list = [None] * num_images\n    elif isinstance(cmaps, str):\n        cmaps_list = [cmaps] * num_images\n    else:\n        cmaps_list = list(cmaps)\n        if len(cmaps_list) < num_images:\n            last_cmap = cmaps_list[-1] if cmaps_list else None\n            cmaps_list.extend([last_cmap] * (num_images - len(cmaps_list)))\n\n    for i in range(len(axes_flat)):\n        if i < num_images:\n            img, title, cmap_val = images[i], titles[i], cmaps_list[i]\n            axes_flat[i].imshow(img, cmap=cmap_val if img.ndim==2 or (img.ndim==3 and img.shape[-1]==1) else None)\n            axes_flat[i].set_title(title)\n            axes_flat[i].axis('off')\n        else:\n            axes_flat[i].axis('off')\n\n    if main_title:\n        fig.suptitle(main_title, fontsize=16)\n\n    plt.tight_layout(rect=[0, 0, 1, 0.96] if main_title else None)\n    plt.show()\n\n# --- Create/Load Example Images ---\n# Image for contours and descriptors\nsimple_shape = np.zeros((100, 100), dtype=np.uint8)\nsimple_shape[20:80, 30:70] = 1 # Rectangle\nsimple_shape[40:60, 50:90] = 1 # Adds a part for an L shape\nsimple_shape_labels = measure.label(simple_shape)\nsimple_shape_props = measure.regionprops(simple_shape_labels)\ncontour_coords = None\nif simple_shape_props: # Get contour of the largest object\n    # Find the largest object (if there are multiple disconnected ones)\n    largest_obj_idx = np.argmax([prop.area for prop in simple_shape_props])\n    contour_coords = measure.find_contours(simple_shape_labels == (largest_obj_idx + 1), 0.5)[0]\n\n\n# Image for texture and regional moments (we will use the brain one)\ntry:\n    brain_volume = data.brain()\n    if brain_volume.ndim == 3:\n        slice_idx = brain_volume.shape[0] // 2\n        brain_slice_c11 = brain_volume[slice_idx, :, :]\n    elif brain_volume.ndim == 2:\n        brain_slice_c11 = brain_volume\n    else: # Fallback if data.brain() is not as expected\n        print(\"Unexpected format for data.brain(), using data.camera()\")\n        brain_slice_c11 = data.camera()\nexcept Exception as e:\n    print(f\"Error loading data.brain(): {e}. Using data.camera().\")\n    brain_slice_c11 = data.camera()\nbrain_slice_c11_ubyte = img_as_ubyte(brain_slice_c11)\n\n\n# Image for template matching\nchecker_c11 = data.checkerboard()\n# Create a template from the image\ntemplate_c11 = checker_c11[20:70, 20:70].copy() # A 50x50 template\n\nprint(\"Example images and shapes are ready.\")\nplot_many_images_c11_12(\n    [simple_shape, brain_slice_c11_ubyte, checker_c11, template_c11],\n    [\"Simple Binary Shape\", \"Brain Slice (Gray)\", \"Checkerboard\", \"Checkerboard Template\"],\n    2, 2, cmaps=['gray','gray','gray','gray']\n)\nif contour_coords is not None:\n    plt.figure(figsize=(5,5))\n    plt.plot(contour_coords[:, 1], contour_coords[:, 0], linewidth=2)\n    plt.gca().invert_yaxis() # The image y-axis is inverted\n    plt.title(\"Simple Shape Contour\")\n    plt.show()",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "LPiRPtTKOE0k",
    "outputId": "7f8a581f-82b2-4617-9391-7cd8b0555fd7"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Interpreting the Results (Initial Setup):\n\n- This block should load the libraries and create/load the example images without errors.\n- You will see: a simple binary shape, a grayscale brain slice, a checkerboard, and a small template taken from the checkerboard.\n- The contour of the simple shape will also be plotted, which we will use for contour descriptors.\n",
   "metadata": {
    "id": "Jx0AxARbOQGt"
   }
  },
  {
   "cell_type": "markdown",
   "source": "## Module 1: Chapter 11\n\nWe will focus on how to represent objects (after segmentation) and describe them quantitatively.\n",
   "metadata": {
    "id": "EKR_oxvXOazL"
   }
  },
  {
   "cell_type": "markdown",
   "source": "### 1.1. Representation:\n**Brief Explanation:**\n\n- Representation: After segmentation, objects can be represented by their external characteristics (contour) or internal characteristics (region).\n- Contour Following: Algorithms used to trace the boundary of an object in a binary image.\n- Chain Codes: Represent a contour as a sequence of small unit-length directional segments (using, for example, 4 or 8 directions). It is a compact representation and invariant to translation.\n\n\n**How the Code Works (Simple Chain Code):**\n\n1. Uses `contour_coords` from the simple shape defined in the setup.\n2. Computes differences between coordinates of consecutive contour pixels.\n3. Maps those differences to a 4-direction chain code (0: right, 1: up, 2: left, 3: down). (For simplicity, we will not implement initial-point or rotation normalization here).\n",
   "metadata": {
    "id": "8OhIcIcCOojL"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 719
    },
    "id": "oe9StECvNgHC",
    "outputId": "97519513-66a7-428c-e47b-8b6a6336da88"
   },
   "outputs": [],
   "source": "# 1.1 Chain Codes (Chain Codes)\nprint(\"\\n--- Module 1.1: Chain Codes ---\")\n\nif contour_coords is not None and len(contour_coords) > 1:\n    chain_code_4dir = []\n    # Contour coordinates are (row, column)\n    # (row decreases = up, row increases = down)\n    # (column decreases = left, column increases = right)\n    # Adjust for image coordinate system (y increases downward)\n    # 0: Right (dx=1, dy=0) -> (0, 1) in (col, row)\n    # 1: Up    (dx=0, dy=-1) -> (-1, 0) in (col, row)\n    # 2: Left (dx=-1, dy=0) -> (0, -1) in (col, row)\n    # 3: Down   (dx=0, dy=1) -> (1, 0) in (col, row)\n\n    # find_contours coords are (row, column)\n    # dy = diff_row, dx = diff_col\n    # 0: Right (d_col=1, d_row=0)\n    # 1: Up    (d_col=0, d_row=-1)\n    # 2: Left (d_col=-1, d_row=0)\n    # 3: Down   (d_col=0, d_row=1)\n\n    for i in range(len(contour_coords) - 1):\n        # diff = contour_coords[i+1] - contour_coords[i]\n        # Correcting to (row, column)\n        # diff_row = contour_coords[i+1, 0] - contour_coords[i, 0]\n        # diff_col = contour_coords[i+1, 1] - contour_coords[i, 1]\n\n        # For chain codes, we typically consider nearest neighbors.\n        # find_contours may return non-adjacent points on a grid.\n        # For a simple example, we will assume points are adjacent\n        # or we will compute the predominant direction.\n        # A robust chain code implementation requires contour points to be sequential and adjacent on the grid.\n        # `measure.find_contours` returns float coordinates, which may not lie perfectly on the grid.\n        # For a didactic example, we will round and compute differences.\n\n        pt_atual = np.round(contour_coords[i]).astype(int)\n        pt_prox = np.round(contour_coords[i+1]).astype(int)\n\n        diff_row = pt_prox[0] - pt_atual[0]\n        diff_col = pt_prox[1] - pt_atual[1]\n\n        code = -1 # Invalid code\n        if diff_col == 1 and diff_row == 0: code = 0 # Right\n        elif diff_col == 0 and diff_row == -1: code = 1 # Up\n        elif diff_col == -1 and diff_row == 0: code = 2 # Left\n        elif diff_col == 0 and diff_row == 1: code = 3 # Down\n        # For 8-connectivity, we would add diagonals\n\n        if code != -1:\n            chain_code_4dir.append(code)\n        # Note: If points are not strictly adjacent on the 4-connected grid,\n        # this simple logic may fail or skip points.\n\n    print(f\"Contour (first 5 points): \\n{np.round(contour_coords[:5]).astype(int)}\")\n    print(f\"4-direction Chain Code (first 20 steps): {chain_code_4dir[:20]}\")\n    print(f\"Chain code length: {len(chain_code_4dir)}\")\n\n    # Visualize the original contour\n    plt.figure(figsize=(6,6))\n    plt.plot(contour_coords[:, 1], contour_coords[:, 0], linewidth=2, label='Original Contour')\n    plt.scatter(contour_coords[0, 1], contour_coords[0, 0], c='red', s=100, label='Starting Point', zorder=5)\n    plt.gca().invert_yaxis()\n    plt.title(\"Simple Shape Contour and Starting Point\")\n    plt.legend()\n    plt.show()\nelse:\n    print(\"Contour not found for 'simple_shape'. Skip this example.\")"
  },
  {
   "cell_type": "markdown",
   "source": "**Interpreting the Results (Chain Codes):**\n\n- Contour: The plot should show the boundary of the \"Simple Binary Shape\" with the starting point marked.\n- Chain Code: The printed list of numbers represents the sequence of directions (0-3) used to trace the contour from the starting point. This is the chain code.\n- Length: The number of steps in the chain code, which is related to the object's perimeter.\n\n\n**Exercises:**\n1. How would you extend the code to generate an 8-direction chain code? What would the new codes and conditions for `diff_row` and `diff_col` be?\n",
   "metadata": {
    "id": "S3BNOqSLP0PW"
   }
  },
  {
   "cell_type": "markdown",
   "source": "## 1.2. Contour Descriptors\n\n**Brief Explanation:** After obtaining the contour, we can describe it using several measurements.\n\n- Simple Descriptors: Perimeter length, diameter, eccentricity.\n- Fourier Descriptors: Represent the contour in the frequency domain. The contour coordinates (x, y) are treated as a complex signal s(k)=x(k)+jy(k). The DFT of s(k) produces the Fourier descriptors. Using a subset of the first coefficients provides a contour approximation and is invariant to some transformations.\n\n\n**How the Code Works** (Fourier Descriptors):\n\n1. Uses `contour_coords` from the simple shape.\n2. Treats contour coordinates as a complex signal: s(k)=column(k)+j⋅row(k).\n3. Computes the 1D DFT of s(k) using `fft()`. The result is the Fourier descriptors.\n4. To reconstruct/approximate the contour, a limited number of the first descriptors is used (low frequencies, which capture the general shape), and then the IDFT is computed.\n5. Displays the original contour and the reconstructed contour with different numbers of descriptors.\n",
   "metadata": {
    "id": "uzUZrc4tQMUW"
   }
  },
  {
   "cell_type": "code",
   "source": "# 1.2 Contour Descriptors (Fourier Descriptors)\nprint(\"\\n--- Module 1.2: Fourier Descriptors ---\")\n\nif contour_coords is not None and len(contour_coords) > 1:\n    # Treat the contour as a complex signal: s(k) = x(k) + j*y(k)\n    # Where x is the column coordinate and y is the row coordinate\n    # In our contour_coords, contour_coords[:, 1] is column (x), contour_coords[:, 0] is row (y)\n    sinal_complexo_contorno = contour_coords[:, 1] + 1j * contour_coords[:, 0]\n\n    # Compute Fourier Descriptors (DFT of complex signal)\n    descritores_fourier = fft(sinal_complexo_contorno)\n\n    # Function to reconstruct the contour using N_desc descriptors\n    def reconstruir_contorno_fourier(descritores, N_desc, tamanho_original):\n        if N_desc <= 0 or N_desc > len(descritores):\n            N_desc = len(descritores) # Use all if N_desc is invalid\n\n        # Keep the first N_desc/2 and last N_desc/2 coefficients (low frequencies)\n        # If N_desc is odd, adjust\n        desc_truncados = np.zeros_like(descritores, dtype=complex)\n\n        # Keep DC component and N_desc-1 lowest frequencies (positive and negative)\n        # The DFT spectrum is typically [DC, F_pos, ..., F_Nyquist, ..., F_neg]\n        # To keep N_desc, we take DC, (N_desc-1)//2 positives, and (N_desc-1)//2 negatives\n        # If N_desc is even, N_desc/2 - 1 positives, N_desc/2 negatives\n        # If N_desc is odd, (N_desc-1)/2 positives, (N_desc-1)/2 negatives\n\n        # A simpler way to truncate to N_desc components is\n        # keep the first N_desc/2 and last N_desc/2 (after DC).\n        # But for rotation and scale invariance, more complex descriptor processing is required.\n        # Here, we will focus on reconstructing the overall shape.\n        # For visual reconstruction, keeping lower frequencies is most important.\n        # fftshift places DC at the center. Without shift, DC is at [0].\n        # Low frequencies are near DC and at array ends (due to periodicity).\n\n        meio_n_desc = N_desc // 2\n        if N_desc == 1: # Only DC\n             desc_truncados[0] = descritores[0]\n        elif N_desc > 1 :\n            desc_truncados[0] = descritores[0] # Keep DC\n            # Keep first (N_desc-1)//2 positive frequencies\n            desc_truncados[1 : (N_desc -1)//2 + 1] = descritores[1 : (N_desc -1)//2 + 1]\n            # Keep last (N_desc-1)//2 negative frequencies (conjugate symmetry)\n            # If N_desc is even, N_desc/2 -1 positives and N_desc/2 negatives\n            # Or more simply: keep the first N_desc and handle symmetry if invariance is needed.\n            # For simple reconstruction:\n            desc_truncados[1:meio_n_desc+1] = descritores[1:meio_n_desc+1]\n            if N_desc > 1: # Avoid negative index if N_desc=1\n                 desc_truncados[-meio_n_desc:] = descritores[-meio_n_desc:]\n\n\n        contorno_reconst = ifft(desc_truncados)\n        # Scale back to original size if IDFT does not do so automatically\n        # (numpy fft/ifft preserve \"size\" in terms of number of points)\n        return np.column_stack((contorno_reconst.imag, contorno_reconst.real)) # (row, column)\n\n    # Reconstruct with different numbers of descriptors\n    N_desc_10 = 10\n    N_desc_30 = 30\n    N_desc_todos = len(descritores_fourier)\n\n    contorno_rec_10 = reconstruir_contorno_fourier(descritores_fourier, N_desc_10, len(sinal_complexo_contorno))\n    contorno_rec_30 = reconstruir_contorno_fourier(descritores_fourier, N_desc_30, len(sinal_complexo_contorno))\n    contorno_rec_todos = reconstruir_contorno_fourier(descritores_fourier, N_desc_todos, len(sinal_complexo_contorno))\n\n    # Visualization\n    fig, axs = plt.subplots(1, 4, figsize=(20, 5), sharex=True, sharey=True)\n    fig.suptitle(\"Fourier Descriptors and Contour Reconstruction\", fontsize=16)\n\n    axs[0].plot(contour_coords[:, 1], contour_coords[:, 0], linewidth=2)\n    axs[0].set_title('Original Contour')\n\n    axs[1].plot(contorno_rec_10[:, 1], contorno_rec_10[:, 0], linewidth=2)\n    axs[1].set_title(f'Reconstructed ({N_desc_10} Descriptors)')\n\n    axs[2].plot(contorno_rec_30[:, 1], contorno_rec_30[:, 0], linewidth=2)\n    axs[2].set_title(f'Reconstructed ({N_desc_30} Descriptors)')\n\n    axs[3].plot(contorno_rec_todos[:, 1], contorno_rec_todos[:, 0], linewidth=2)\n    axs[3].set_title(f'Reconstructed (All {N_desc_todos} Desc.)')\n\n    for ax in axs:\n        ax.invert_yaxis() # Adjust y-axis orientation\n        ax.set_aspect('equal', adjustable='box')\n    plt.show()\nelse:\n    print(\"Contour not found or too small. Skip the Fourier Descriptor example.\")",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 458
    },
    "id": "dF_grIZPPIO7",
    "outputId": "64c018d3-c77d-4342-908c-b605d0e6fa7a"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "**Interpreting the Results (Fourier Descriptors):**\n\n* Original Contour: The original shape.\n* Reconstructed (10 Descriptors): Using only the first 10 Fourier descriptors (which represent the lowest-frequency components of the contour shape), the reconstruction will be a smoothed, approximate version of the original shape, capturing general characteristics but losing fine details.\n* Reconstructed (30 Descriptors): With more descriptors, the reconstruction will be more faithful to the original shape, including more details.\n* Reconstructed (All Descriptors): Should be (almost) identical to the original contour.\n\n**Exercises**\n\n1. Try different values for `N_desc` in the `reconstruir_contorno_fourier` function. What is the minimum number of descriptors that still provides a visually recognizable representation of the original shape?\n2. Fourier descriptors are translation invariant. To make them scale- and rotation-invariant, they are usually further processed (e.g., using only descriptor magnitudes or normalizing by the first non-zero descriptor). Briefly research how rotation and scale invariance can be achieved. (This is more conceptual).\n",
   "metadata": {
    "id": "WFCjFuNeRmEd"
   }
  },
  {
   "cell_type": "markdown",
   "source": "## 1.3. Regional Descriptors\n\n**Brief Explanation:** They describe an entire region (not only its contour).\n\n* Topological Descriptors: E.g., Euler number (number of objects - number of holes).\n* Texture: Measures of smoothness, roughness, and regularity. Statistical (histogram, GLCM), structural, and spectral approaches. We will focus on some simple statistics.\n* Moments: Geometric and invariant moments (Hu moments) describe region shape and are invariant to translation, scale, and rotation.\n\n**How the Code Works:**\n\n1. Uses `brain_slice_c11_ubyte` and `simple_shape` (filled).\n1. Euler Number: `measure.euler_number()`.\n1. Texture (Simple Statistics): Computes mean, standard deviation, smoothness (based on variance), uniformity, and entropy from the histogram of a region in the brain image.\n1. Hu Moments: `cv2.moments()` computes spatial moments. `cv2.HuMoments()` computes the 7 Hu moments from spatial moments. These are applied to the filled simple shape.\n",
   "metadata": {
    "id": "NIzh2EKQR9Cd"
   }
  },
  {
   "cell_type": "code",
   "source": "from skimage.exposure import rescale_intensity, histogram\n\n# 1.3 Regional Descriptors\nprint(\"\\n--- Module 1.3: Regional Descriptors ---\")\n\nif 'brain_slice_c11_ubyte' in globals() and 'simple_shape' in globals():\n    # --- a) Topological Descriptors (Euler Number) ---\n    # For Euler Number, we need a clean binary image.\n    # Use 'simple_shape' and ensure it is filled.\n    forma_preenchida_euler = ndimage.binary_fill_holes(simple_shape > 0)\n    # euler_number counts objects - holes. For a solid shape, it should be 1.\n    # For skimage < 0.19, connectivity is an argument. For >= 0.19, it is not.\n    try:\n        num_euler = measure.euler_number(forma_preenchida_euler, connectivity=2)\n    except TypeError: # Older versions may not have connectivity or may handle it differently\n         try:\n            num_euler = measure.euler_number(forma_preenchida_euler) # Try without it\n         except Exception as e_euler:\n            print(f\"Error calculating Euler number: {e_euler}\")\n            num_euler = \"Erro\"\n\n    print(f\"Euler number for filled 'simple_shape': {num_euler}\")\n\n    # --- b) Texture (Simple Statistical Descriptors) ---\n    # Get a sub-region of the brain image for texture analysis\n    # (e.g., a region that appears textured)\n    regiao_textura = brain_slice_c11_ubyte[100:150, 100:150]\n    if regiao_textura.size == 0: # Fallback if crop is empty\n        regiao_textura = brain_slice_c11_ubyte[0:50,0:50]\n\n    if regiao_textura.size > 0:\n        media_textura = np.mean(regiao_textura)\n        std_textura = np.std(regiao_textura)\n        # Smoothness (related to variance) R = 1 - 1/(1+sigma^2)\n        # Normalize std to [0,1] if image is in [0,255]\n        std_norm_textura = std_textura / 255.0\n        suavidade_textura = 1 - (1 / (1 + std_norm_textura**2))\n\n        # Histogram Uniformity and Entropy\n        hist_textura, _ = histogram(regiao_textura, nbins=256, source_range='image')\n        prob_textura = hist_textura / np.sum(hist_textura) # Normalize to make it a PDF\n        uniformidade_textura = np.sum(prob_textura**2)\n        entropia_textura = -np.sum(prob_textura * np.log2(prob_textura + 1e-8)) # Add epsilon to avoid log(0)\n\n        print(f\"\\nTexture Descriptors for selected region:\")\n        print(f\"  Mean: {media_textura:.2f}\")\n        print(f\"  Standard Deviation: {std_textura:.2f}\")\n        print(f\"  Smoothness (R): {suavidade_textura:.4f}\")\n        print(f\"  Uniformity: {uniformidade_textura:.4f}\")\n        print(f\"  Entropy: {entropia_textura:.4f}\")\n    else:\n        print(\"Texture region is empty.\")\n\n    # --- c) Geometric and Hu Invariant Moments ---\n    # Use OpenCV to compute moments. Requires ubyte image and single contour.\n    # Input image for cv2.moments must be binary (0 or 255) or grayscale.\n    # Use 'forma_preenchida_euler', which is binary (0 or 1), convert to ubyte.\n    forma_para_momentos_ubyte = img_as_ubyte(forma_preenchida_euler)\n\n    momentos_espaciais = cv2.moments(forma_para_momentos_ubyte)\n    # print(f\"\\nSpatial Moments (dictionary): {momentos_espaciais}\")\n\n    # Compute Hu Moments (invariant to translation, scale, rotation)\n    momentos_hu = cv2.HuMoments(momentos_espaciais).flatten() # flatten to 1D array\n    # Apply log for better visualization/comparison of magnitude orders\n    momentos_hu_log = -np.sign(momentos_hu) * np.log10(np.abs(momentos_hu) + 1e-8)\n\n\n    print(f\"\\nHu Moments (log-transformed):\")\n    for i, mom_hu in enumerate(momentos_hu_log):\n        print(f\"  hu[{i+1}]: {mom_hu:.4f}\")\n\n    # Visualization\n    plot_many_images_c11_12([forma_preenchida_euler, regiao_textura],\n                           [f\"Shape for Euler ({num_euler}) and Moments\", \"Region for Texture\"],\n                           1, 2, cmaps=['gray','gray'], figsize=(10,5),\n                           main_title=\"Regional Descriptors\")\nelse:\n    print(\"Images 'brain_slice_c11_ubyte' or 'simple_shape' are not defined.\")",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 842
    },
    "id": "NfWKPRONScnQ",
    "outputId": "047da098-8331-479a-969d-43642b3cfdfb"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "**Interpreting the Results (Regional Descriptors):**\n\n- Euler Number: For `forma_preenchida_euler` (a solid object), the Euler number should be 1 (1 object - 0 holes).\n\n- Mean: Average gray level of the region.\n- Standard Deviation: Measure of gray-level dispersion (local contrast).\n- Smoothness (R): Close to 1 for smooth regions (low variance), close to 0 for less smooth regions.\n- Uniformity: High for regions with little intensity variation (histogram with a few tall peaks).\n- Entropy: High for regions with a more spread-out histogram (complexity/randomness).\n\n\n- Hu Moments: The 7 printed values are Hu moments (log-transformed for easier visualization, since their magnitudes can vary a lot). They characterize object shape. If you rotate, scale, or translate `forma_preenchida_euler` and recompute Hu moments, they should remain (approximately) the same.\n\n\n**Exercise:**\n\n1. Texture: Select a different region of the brain image (`brain_slice_c11_ubyte`) with a visually distinct texture from the first one (e.g., a darker and more homogeneous region vs. a brighter and more textured one). Recompute and compare statistical texture descriptors. Do they reflect the visual differences?\n2. Hu Moments:\n- Create a new simple binary shape (e.g., a circle or triangle).\n- Compute its Hu moments.\n- Now rotate or slightly scale this new shape (using `skimage.transform.rotate` or `skimage.transform.rescale`) and recompute Hu moments. Are they truly invariant? (Small variations may occur due to discretization).\n",
   "metadata": {
    "id": "8F78iKAsUbcq"
   }
  },
  {
   "cell_type": "markdown",
   "source": "## Chapter 12 - Object Recognition\n\n\nWe will focus on methods based on theoretical decision principles: template matching and an introduction to statistical classifiers and neural networks.\n",
   "metadata": {
    "id": "POXVu6r3VbW5"
   }
  },
  {
   "cell_type": "markdown",
   "source": "### 2.1. Patterns, Classes, and Template Matching\n\n**Brief Explanation:**\n\n- Object Recognition: Assigning a label (class) to an object based on its descriptors (features).\n- Patterns and Classes: A pattern is an arrangement of descriptors. A class is a set of patterns with common properties.\n- Template Matching: A simple method to find occurrences of a pattern (template) in a larger image. The template is slid over the image, and a similarity (or difference) measure is computed at each position.\n\n\n**How the Code Works (Template Matching):**\n\n1. Uses `checker_c11` as the main image and `template_c11` (a checkerboard crop) as the template.\n1. `feature.match_template(image, template, pad_input=True)`: Computes normalized correlation response (or normalized squared difference, depending on internals, but the result is a similarity measure) between the template and all image windows.\n* `pad_input=True`: Adds padding to the image so the template can be matched at the borders.\n3. Find the Best Match: `np.unravel_index(np.argmax(resultado_matching), resultado_matching.shape)` finds the coordinates (row, column) where matching response (similarity) is maximum.\n4. Visualization: Shows the image, template, and image with a rectangle drawn at the best match.\n",
   "metadata": {
    "id": "qlkwKzRnViFr"
   }
  },
  {
   "cell_type": "code",
   "source": "# 2.1 Template Matching\nprint(\"\\n--- Module 2.1: Template Matching ---\")\n\nif 'checker_c11' in globals() and 'template_c11' in globals():\n    # The image and template must be grayscale for skimage match_template\n    # If they were colored, we would convert them or perform channel-wise matching.\n    # checker_c11 and template_c11 are already grayscale (or can be converted if needed).\n    # Ensure they are float for match_template\n    img_for_match = img_as_float(checker_c11)\n    template_for_match = img_as_float(template_c11)\n\n    # Perform template matching\n    # The function returns a similarity map\n    matching_result = feature.match_template(img_for_match, template_for_match, pad_input=True)\n\n    # Find the best match position (maximum value in the similarity map)\n    ij = np.unravel_index(np.argmax(matching_result), matching_result.shape)\n    x_match, y_match = ij[::-1] # Coordinates (column, row) of the top-left corner of the best match\n\n    # Template dimensions\n    h_temp, w_temp = template_for_match.shape\n\n    # Create a figure for drawing\n    fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n    ax.imshow(checker_c11, cmap='gray')\n    ax.set_title('Template Matching Result')\n    ax.axis('off')\n\n    # Create a rectangle to highlight the best match\n    # match_template result may place the \"hot spot\" at the corner or center of the template.\n    # For `pad_input=True`, the output has the same size as the image and `ij` is the top-left corner.\n    rect = plt.Rectangle((x_match, y_match), w_temp, h_temp, edgecolor='r', facecolor='none', linewidth=2)\n    ax.add_patch(rect)\n    plt.show()\n\n    print(f\"Template found at corner position (column, row): ({x_match}, {y_match})\")\n\n    plot_many_images_c11_12([checker_c11, template_c11, matching_result],\n                           [\"Main Image\", \"Template\", \"Similarity Map (Matching Result)\"],\n                           1,3, cmaps=['gray','gray','viridis'], figsize=(15,5))\nelse:\n    print(\"Images 'checker_c11' or 'template_c11' are not defined.\")",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "2iKHrhAzQiW2",
    "outputId": "4fddf32d-96fd-4485-f7a7-f29bb219f09f"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "**Interpreting the Results (Template Matching):**\n\n- Main Image and Template: The input images.\n- Similarity Map: An image where each pixel represents how similar that pixel's neighborhood is to the template. Bright points indicate high similarity.\n- Visual Result (with rectangle): The main image with a red rectangle drawn around the region identified by the algorithm as the best match for the template.\n\n\n**Exercise:**\n\n1. Create a new `template_novo` from a different part of `checker_c11`. Does the algorithm still find it?\n1. Try using a template that is not present in `checker_c11` (you can create a small random NumPy array or load another small image). What happens to the \"similarity map\" and the \"best match\" found?\n1. If you rotate or scale the template, will `skimage`'s `match_template` (which is based on normalized correlation) still work well?\n",
   "metadata": {
    "id": "s_dpndmOJIIU"
   }
  },
  {
   "cell_type": "markdown",
   "source": "## 2.2. Optimal Statistical Classifiers (Bayes)\n\n**Brief Explanation:** Bayesian decision theory provides a framework for building classifiers that minimize error probability (Bayes classifier).\n\nGiven an object feature vector **x**, we want to assign it to class ω_{i} that maximizes posterior probability P(ω_{i}∣x).\nBy Bayes' theorem: P(ω_{i}∣x)=p(x∣ω_{i})P(ω_{i})/p(x), where:\n- p(x∣ω_{i}): Conditional probability density of features (likelihood).\n- P(ω_{i}): Prior probability of class ω_{i}.\n- p(x): Evidence (normalization constant).\n\nDecision: Choose ω_{i} if p(x∣ω_{i})P(ω_{i})>p(x∣ω_{j})P(ω_{j}) for every j!=i.\n\nIf PDFs p(x∣ω_{i}) are Gaussian, decision surfaces can be linear or quadratic.\n\n\n**How the Code Works (Simplified Pixel Classification Example):**\n1. Generate Synthetic Data: Creates two clouds of 2D points (representing features of two pixel classes, e.g., intensity and local texture) that follow Gaussian distributions with different means and covariances.\n1. Train a Gaussian Bayesian Classifier: `sklearn.naive_bayes.GaussianNB` is a simple classifier that assumes independence between features (naive) and Gaussian distribution for each feature within each class.\n1. Create a Grid to Visualize the Decision: Generates a grid of points in feature space.\n1. Predict Classes on the Grid: The trained classifier predicts the class for each grid point.\n1. Visualization: Plots the original training data and classifier decision region.\n",
   "metadata": {
    "id": "FNspE7puj9Iy"
   }
  },
  {
   "cell_type": "code",
   "source": "# 2.2 Simple Gaussian Bayesian Classifier\nprint(\"\\n--- Module 2.2: Gaussian Bayesian Classifier (Simple) ---\")\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.datasets import make_blobs # To generate synthetic data\n\n# 1. Generate synthetic data for two Gaussian-distributed classes\n#    Each \"sample\" can represent a pixel described by 2 features\nn_amostras_bayes = 300\ncentros_bayes = [(-1, -1), (2, 2)] # Means das duas classes\nstd_bayes = 0.8 # Class standard deviation\nX_bayes, y_bayes = make_blobs(n_samples=n_amostras_bayes, centers=centros_bayes,\n                              cluster_std=std_bayes, random_state=42)\n\n# 2. Train a Gaussian Naive Bayes classifier\ngnb = GaussianNB()\ngnb.fit(X_bayes, y_bayes)\n\n# 3. Create a grid to visualize decision boundary\nx_min_bayes, x_max_bayes = X_bayes[:, 0].min() - 1, X_bayes[:, 0].max() + 1\ny_min_bayes, y_max_bayes = X_bayes[:, 1].min() - 1, X_bayes[:, 1].max() + 1\nxx_bayes, yy_bayes = np.meshgrid(np.linspace(x_min_bayes, x_max_bayes, 100),\n                                 np.linspace(y_min_bayes, y_max_bayes, 100))\n\n# 4. Predict classes for each grid point\nZ_bayes = gnb.predict(np.c_[xx_bayes.ravel(), yy_bayes.ravel()])\nZ_bayes = Z_bayes.reshape(xx_bayes.shape)\n\n# 5. Visualization\nplt.figure(figsize=(8, 6))\nplt.contourf(xx_bayes, yy_bayes, Z_bayes, alpha=0.4, cmap='coolwarm')\nplt.scatter(X_bayes[:, 0], X_bayes[:, 1], c=y_bayes, s=20, edgecolor='k', cmap='coolwarm')\nplt.title(\"Gaussian Naive Bayes Classifier - Decision Regions\")\nplt.xlabel(\"Feature 1\")\nplt.ylabel(\"Feature 2\")\nplt.grid(True)\nplt.show()\n\n# Image application example (conceptual, since we would train with real data)\n# Take two regions from the brain image representing two classes (e.g., white matter, gray matter)\nif 'brain_slice_c11_ubyte' in globals():\n    img_cerebro_bayes = brain_slice_c11_ubyte\n    # Very simplified example: use intensity as the only feature\n    # We could take samples from two regions and their intensities\n    # Region 1 (e.g., darker)\n    # amostras_r1 = img_cerebro_bayes[100:120, 100:120].ravel()\n    # Region 2 (e.g., brighter)\n    # amostras_r2 = img_cerebro_bayes[150:170, 150:170].ravel()\n    # ... train gnb with these samples ...\n    # ... then classify all pixels in image_cerebro_bayes ...\n    print(\"To apply this to a real image, we would need to extract features from known regions,\")\n    print(\"train the classifier, and then apply it to the entire image.\")\nelse:\n    print(\"Brain image not available for conceptual Bayes example.\")",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 633
    },
    "id": "JQ1D1-rGj8un",
    "outputId": "dd8f4548-4194-467c-88cc-75f171a73f24"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "**Interpreting the Results (Simple Bayes):**\n\n- The plot shows two point clouds representing samples from two classes (blue and red).\n- The colored background area (light blue and light red) represents decision regions learned by the Gaussian Naive Bayes classifier. If a new sample falls in the light blue region, it will be classified as class 0 (blue); if it falls in the light red region, class 1 (red).\n- The line (or curve) separating the two colored regions is the decision boundary. For Gaussian Naive Bayes with class-specific covariances (default), this boundary can be quadratic. If covariances were assumed equal, it would be linear.\n\n\n\n**Exercise:**\n\n1. In `make_blobs`, change `centros_bayes` so that classes are closer together or more overlapping. Re-run the code. How does the decision boundary change? Is separation still good?\n1. Change `std_bayes` (cluster standard deviation) to a larger value (e.g., 1.5). How does this affect class overlap and decision boundary?\n1. If you had a medical image and wanted to segment two tissue types with this classifier, what pixel features (besides intensity) could you use to form feature vector x?\n\n",
   "metadata": {
    "id": "GfpiJzRrmHXR"
   }
  },
  {
   "cell_type": "markdown",
   "source": "## 2.3. Artificial Neural Networks (Introduction)\n\n**Brief Explanation:** Artificial Neural Networks (ANNs), especially Multi-Layer Perceptrons (MLPs), are powerful classifiers that can learn complex, non-linear decision boundaries.\n\n- Basic Structure: Input layer (features), one or more hidden layers (with neurons and non-linear activation functions such as ReLU or sigmoid), and an output layer (classes).\n- Training: The process of adjusting connection weights between neurons using an algorithm such as backpropagation to minimize an error on a training dataset.\n\n\n**How the Code Works (Simple MLP with scikit-learn):**\n\n1. Data: Reuses synthetic data `X_bayes`, `y_bayes` from the previous example.\n1. Train/Test Split: `train_test_split` divides data into training and test sets to evaluate model generalization.\n1. Feature Scaling: `StandardScaler` normalizes features (mean 0, variance 1). This is often important for good neural network performance.\n\n1. MLP Creation and Training:\n\n`MLPClassifier(hidden_layer_sizes=(10, 5), ...)`: Defines an MLP with two hidden layers, the first with 10 neurons and the second with 5.\n- `activation='relu'`: ReLU activation function.\n- `solver='adam'`: Optimizer for weight adjustment.\n- `max_iter`: Maximum number of training epochs.\n- `random_state`: For reproducibility.\n- `mlp.fit(X_train_scaled, y_train)`: Trains the model.\n\n5. Prediction and Evaluation:\n`mlp.predict(X_test_scaled)`: Makes predictions on the test set.\n`accuracy_score()`: Computes accuracy.\n6. Visualization: Plots data and decision boundary learned by the MLP.\n",
   "metadata": {
    "id": "lgCLn9OZmt2s"
   }
  },
  {
   "cell_type": "code",
   "source": "# 2.3 Artificial Neural Networks (MLP Simples com scikit-learn)\nprint(\"\\n--- Module 2.3: Neural Networks (Basic MLP) ---\")\n\nif 'X_bayes' in globals() and 'y_bayes' in globals():\n    # 1. Split data into training and testing\n    X_train, X_test, y_train, y_test = train_test_split(X_bayes, y_bayes, test_size=0.3, random_state=42)\n\n    # 2. Scale features (important for Neural Networks)\n    scaler = StandardScaler()\n    X_train_scaled = scaler.fit_transform(X_train)\n    X_test_scaled = scaler.transform(X_test)\n\n    # 3. Create and Train the MLPClassifier\n    #    hidden_layer_sizes: tuple, e.g., (100,) for one hidden layer with 100 neurons\n    #                          e.g., (50, 20) for two hidden layers\n    mlp = MLPClassifier(hidden_layer_sizes=(10, 5), # Two hidden layers\n                        activation='relu',        # Activation function\n                        solver='adam',            # Optimizer\n                        max_iter=1000,            # Maximum training iterations\n                        random_state=42,\n                        early_stopping=True,      # Stop early if no improvement\n                        n_iter_no_change=20)      # Patience for early stopping\n\n    print(\"Training MLP...\")\n    mlp.fit(X_train_scaled, y_train)\n    print(\"MLP training completed.\")\n\n    # 4. Make predictions and evaluate\n    y_pred_mlp = mlp.predict(X_test_scaled)\n    acuracia_mlp = accuracy_score(y_test, y_pred_mlp)\n    print(f\"MLP accuracy on test set: {acuracia_mlp:.4f}\")\n\n    # 5. Visualize decision boundary (similar to Bayes)\n    # Recompute grid based on scaled data for correct plotting\n    # Or, more simply, use the unscaled grid and transform grid points for prediction\n    xx_mlp_plot, yy_mlp_plot = xx_bayes, yy_bayes # Use the same grid as Bayes example\n\n    # To predict on the grid, we need to scale it the same way as training data\n    grid_points_flat = np.c_[xx_mlp_plot.ravel(), yy_mlp_plot.ravel()]\n    grid_points_flat_scaled = scaler.transform(grid_points_flat) # Use the same trained scaler\n    Z_mlp = mlp.predict(grid_points_flat_scaled)\n    Z_mlp = Z_mlp.reshape(xx_mlp_plot.shape)\n\n    plt.figure(figsize=(8, 6))\n    plt.contourf(xx_mlp_plot, yy_mlp_plot, Z_mlp, alpha=0.4, cmap='coolwarm')\n    # Plot original (unscaled) data for correct axes\n    plt.scatter(X_bayes[:, 0], X_bayes[:, 1], c=y_bayes, s=20, edgecolor='k', cmap='coolwarm')\n    plt.title(f\"MLP Classifier - Decision Regions (Accuracy: {acuracia_mlp:.2f})\")\n    plt.xlabel(\"Feature 1\")\n    plt.ylabel(\"Feature 2\")\n    plt.grid(True)\n    plt.show()\nelse:\n    print(\"Data 'X_bayes', 'y_bayes' not defined. Skip this example.\")",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 652
    },
    "id": "QVCXojo9j6Hg",
    "outputId": "d7a9360c-ad93-460b-d68d-019e02b0ddb0"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "**Interpreting the Results (Simple MLP):**\n\n- Accuracy: The printed value indicates the percentage of samples in the test set that were correctly classified by the trained MLP.\n- Decision Regions: The plot shows input data and decision regions learned by the MLP. Unlike Naive Bayes (which assumes specific distributions and independence), the MLP can learn more complex and non-linear decision boundaries, which may be visible if data clusters are not linearly separable or have complex shapes. The boundary can be more \"flexible.\"\n\n\n**Exercise:**\n\n1. Change the MLP architecture in `hidden_layer_sizes`:\n- Try a single hidden layer with more neurons: `(50,)`.\n- Try more layers or more neurons: `(20, 10, 5)`.\n- How does architecture affect accuracy and decision boundary complexity? (You may need to increase `max_iter` for larger networks to converge).\n2. Change activation function to `'tanh'` or `'logistic'`. Is there any noticeable change in decision boundary or accuracy for this simple problem?\n3. To use an MLP to segment a medical image (e.g., tumor vs. healthy tissue), which features (descriptors from Chapter 11) would you extract from image regions to feed into the network input layer?\n",
   "metadata": {
    "id": "1sD7wdEJnraV"
   }
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {
    "id": "sqYkcBiHoAqj"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}