{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# Python Practice: Blood Cell Classification System\n\n**Objective:** Build a simplified system that can extract features from blood cell images and use those features to train a machine learning classifier capable of distinguishing among different cell types.\n\n**General Instructions:**\n\n- Open this notebook in Google Colab.\n- Run the code cells in order. This project is sequential.\n- Discuss the questions in each section with your group.\n- Modify the code in the exercises to experiment and understand the concepts.\n",
   "metadata": {
    "id": "3x23sHAcWy0Z"
   }
  },
  {
   "cell_type": "markdown",
   "source": "## Part 0: Setup and Data Loading\n**Brief Explanation:**\nLet’s begin by installing and importing the required libraries. We will use the `BloodMNIST` dataset from the MedMNIST suite, which contains images of 8 different blood cell types. The code below will download the data and prepare the images and labels for use.\n\n**How the Code Works:**\n\n1. Installation: `pip install medmnist` installs the library that simplifies dataset downloading.\n2. Imports: Imports all libraries we will use throughout this practice.\n3. Download and Loading:\n- `BloodMNIST(split='train', download=True)`: Downloads the dataset (if not already present) and loads it.\n- We extract images (`images`) and labels (`labels`).\n- `info['label']`: Contains mapping from numeric labels (0-7) to cell class names.\n4. Visualization: A `plot_many_images` function is defined to display a batch of sample images from different classes.\n",
   "metadata": {
    "id": "awqj7AMcXGmo"
   }
  },
  {
   "cell_type": "code",
   "source": "# --- 0.1 Install medmnist library ---\n!pip install -q medmnist",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZPVO2XM9hQKH",
    "outputId": "d3811eb0-a2c9-4644-a513-b7edc83f3760"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 833
    },
    "id": "3HHEJicihJgL",
    "outputId": "642c4781-be8a-4524-bd62-5dc4791f95b9"
   },
   "outputs": [],
   "source": "# --- 0.2 Required Imports ---\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom medmnist import BloodMNIST\nfrom skimage import img_as_float, exposure, filters, morphology, measure, color, transform\nfrom sklearn.model_selection import train_test_split\nfrom skimage.color import rgb2gray, rgb2hsv, hsv2rgb, label2rgb\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\nimport pandas as pd\nfrom skimage import util\nfrom skimage import data, transform, img_as_float, img_as_ubyte, exposure, filters, morphology, feature\nfrom scipy import ndimage\nimport cv2\n\n# --- 0.3 Helper Plotting Function ---\ndef plot_many_images(images, titles, rows=3, cols=4, figsize=(15,10)):\n    fig, axes = plt.subplots(rows, cols, figsize=figsize, squeeze=False)\n    axes_flat = axes.ravel()\n    for i in range(len(axes_flat)):\n        if i < len(images):\n            axes_flat[i].imshow(images[i], cmap='gray' if images[i].ndim==2 else None)\n            axes_flat[i].set_title(titles[i])\n            axes_flat[i].axis('off')\n        else:\n            axes_flat[i].axis('off')\n    plt.tight_layout()\n    plt.show()\n\n# --- 0.4 Load Dataset ---\nprint(\"Downloading and loading BloodMNIST dataset...\")\ntry:\n    dataset = BloodMNIST(split='train', download=True)\n    images = dataset.imgs\n    labels = dataset.labels.flatten()\n    class_names = dataset.info['label']\n    print(\"Dataset loaded successfully!\")\n    print(\"Class Mapping:\", class_names)\n\n    # Show some sample images\n    sample_indices = [np.where(labels == i)[0][0] for i in range(len(class_names))]\n    sample_images = images[sample_indices]\n    sample_titles = [f\"Class {i}: {class_names[str(i)]}\" for i in range(len(class_names))]\n    plot_many_images(sample_images, sample_titles, rows=2, cols=4)\n\nexcept Exception as e:\n    print(f\"Error downloading/loading dataset: {e}\")\n    print(\"The practice cannot continue without data.\")"
  },
  {
   "cell_type": "markdown",
   "source": "**Interpreting the Results (Part 0):**\n\n- The cell should download the dataset and print the class mapping (e.g., '0': 'basophil', '1': 'eosinophil', etc.).\n- A grid with 8 images will be displayed, showing one example of each blood cell type. Observe their visual differences (size, shape, nucleus color, etc.).\n",
   "metadata": {
    "id": "gx4z8mR2XvIi"
   }
  },
  {
   "cell_type": "markdown",
   "source": "## Part 1: Image Fundamentals and Simple Preprocessing\n\n\n**Brief Explanation:**\nEvery analysis starts with understanding the data. We will analyze basic image properties and apply preprocessing techniques (enhancement and noise reduction) to prepare images for segmentation.\n\n**How the Code Works:**\n\n1. Image Analysis: We select an example image and check its dimensions, data type, and bit depth.\n1. Grayscale Conversion: We convert the RGB image to grayscale (`rgb2gray`), since many segmentation and morphology techniques operate on single-channel images.\n1. Enhancement: We apply `exposure.equalize_hist` to improve grayscale contrast.\n1. Restoration: We simulate adding Gaussian noise and apply a median filter to remove it, showing why noise handling matters.\n",
   "metadata": {
    "id": "4MUQ3MQFYALo"
   }
  },
  {
   "cell_type": "code",
   "source": "# --- 1.1 Basic Analysis and Preprocessing ---\n\ndef plot_many_images(images, titles, rows=1, cols=None, cmaps=None, figsize=(15,10), main_title=None):\n    \"\"\"\n    Plots a list of images in a grid.\n\n    Args:\n        images (list): List of images (NumPy arrays).\n        titles (list): List of titles for each image.\n        rows (int): Number of rows in subplot grid.\n        cols (int): Number of columns in grid. If None, calculated automatically.\n        cmaps (list ou str, optional): List of colormaps. If a string, it is used for all.\n                                      If None, matplotlib default is used.\n        figsize (tuple): Figure size.\n        main_title (str, optional): Optional main title for the figure.\n    \"\"\"\n    num_images = len(images)\n    if cols is None:\n        # Calcula colunas para caberem todas as imagens\n        cols = (num_images + rows - 1) // rows\n\n    # squeeze=False garante que 'axes' seja sempre um array 2D, evitando erros\n    fig, axes = plt.subplots(rows, cols, figsize=figsize, squeeze=False)\n    axes_flat = axes.ravel() # Flatten Axes array to simplify iteration\n\n    # Handle cmaps argument flexibly\n    if cmaps is None:\n        cmaps_list = [None] * num_images # Deixa o matplotlib decidir o cmap\n    elif isinstance(cmaps, str):\n        cmaps_list = [cmaps] * num_images # Usa o mesmo cmap para todas\n    else: # Assume it is a list of cmaps\n        cmaps_list = list(cmaps)\n        # If cmaps list is shorter than number of images, fill the rest with None\n        if len(cmaps_list) < num_images:\n            cmaps_list.extend([None] * (num_images - len(cmaps_list)))\n\n    # Iterate over axes (subplots) and plot images\n    for i in range(len(axes_flat)):\n        ax = axes_flat[i]\n        if i < num_images:\n            img = images[i]\n            title = titles[i]\n            cmap_val = cmaps_list[i]\n\n            # imshow handles color images (RGB/RGBA) without cmap.\n            # For 2D images (grayscale), cmap is applied.\n            if img.ndim == 2:\n                ax.imshow(img, cmap=cmap_val)\n            else:\n                ax.imshow(img) # For color images, cmap is ignored\n\n            ax.set_title(title)\n            ax.axis('off')\n        else:\n            ax.axis('off') # Turn off extra axes that will not be used\n\n    if main_title:\n        fig.suptitle(main_title, fontsize=16)\n\n    # Adjust layout to avoid title overlap\n    fig.tight_layout(rect=[0, 0, 1, 0.96] if main_title else None)\n    plt.show()\n\n# Select a sample image for analysis\nidx_analise = 5 # Choose any index from 0 to dataset size\nimg_rgb_exemplo = images[idx_analise]\nlabel_exemplo = labels[idx_analise]\nprint(f\"Analyzing Image {idx_analise} - Class: {class_names[str(label_exemplo)]}\")\n\n# Task 1.1: Fundamentos\nprint(f\"Image dimensions: {img_rgb_exemplo.shape}\")\nprint(f\"Data type: {img_rgb_exemplo.dtype}\")\nprint(f\"Bit depth (per channel): {img_rgb_exemplo.dtype.itemsize * 8} bits\")\n\n# Conversion to Grayscale\nimg_cinza_exemplo = rgb2gray(img_rgb_exemplo)\n\n# Task 1.2: Contrast Enhancement\nimg_eq_exemplo = exposure.equalize_hist(img_cinza_exemplo)\n\n# Task 1.3: Noise Reduction\n# Add noise for simulation and then remove\nimg_ruidosa_exemplo = util.random_noise(img_cinza_exemplo, mode='gaussian', var=0.01)\nimg_denoised_exemplo = filters.median(img_as_ubyte(img_ruidosa_exemplo), footprint=morphology.disk(1))\n\n# Step-by-step Visualization\nplot_many_images(\n    [img_rgb_exemplo, img_cinza_exemplo, img_eq_exemplo, img_ruidosa_exemplo, img_denoised_exemplo],\n    [\"Original RGB\", \"Grayscale\", \"Realce (Hist. Eq.)\", \"With Simulated Noise\", \"After Median Filter\"],\n    rows=2, cols=3, cmaps=[None, 'gray', 'gray', 'gray', 'gray'], figsize=(18,10)\n)",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 901
    },
    "id": "LDKExczviZwP",
    "outputId": "d20dc083-7f8a-496b-9eb3-65ac30e8e78e"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "**Interpreting the Results (Part 1):**\n\n- Grayscale: The black-and-white version of the cell image.\n- Enhancement (Hist. Eq.): The image with improved contrast. Differences between nucleus and cytoplasm may become more visible.\n- With Simulated Noise / After Median Filter: Shows how noise can degrade an image and how a median filter can effectively clean it.\n\n\n**Exercise (Part 1):**\n\n1.  Fundamentals: If the images were 14×14 pixels instead of 28×28, how would that affect your ability to visually distinguish different cell types? Discuss the importance of spatial resolution.\n2. Enhancement: In this example, we used histogram equalization. Try applying a gamma transform (`exposure.adjust_gamma`) to `img_cinza_exemplo`. Would `γ<1` or `γ>1` better highlight the cell nucleus?\n3. Restoration: Replace `filters.median` with `filters.gaussian`. Which filter seems to preserve cell boundaries better while removing noise? Why?\n",
   "metadata": {
    "id": "cCWMptWnYqBT"
   }
  },
  {
   "cell_type": "markdown",
   "source": "## Part 2: Cell Segmentation (Ch. 10)\n**Brief Explanation:**\nThe most critical step is segmentation, which isolates the target cell from the image background. We will use a combination of techniques: thresholding to create an initial mask and mathematical morphology to clean and refine that mask.\n\n**How the Code Works:**\n\n1. Otsu Thresholding (Ch. 10.3): `filters.threshold_otsu()` is applied to the grayscale image (after slight Gaussian blur to reduce noise) to find an optimal global threshold separating cell (usually darker) from background. This creates an initial binary mask.\n2. Morphology for Cleaning (Ch. 9):\n- `remove_small_objects()`: Removes small white \"noise\" objects captured by thresholding.\n- `ndi.binary_fill_holes()`: Fills holes inside the main object (e.g., brighter nucleus parts classified as background).\n",
   "metadata": {
    "id": "8eLWGduHZK3_"
   }
  },
  {
   "cell_type": "code",
   "source": "# --- 2.1 Cell Segmentation (Cap. 10) ---\n\n# Usaremos a imagem em tons de cinza 'img_cinza_exemplo'\nprint(f\"Segmenting image from class: {class_names[str(label_exemplo)]}\")\n\n# A slight Gaussian blur can make thresholding more robust\nimg_suavizada_seg = filters.gaussian(img_cinza_exemplo, sigma=1)\n\n# Task 2.1: Otsu Thresholding (Cap. 10)\nlimiar_otsu_celula = filters.threshold_otsu(img_suavizada_seg)\n# The cell is darker than the background, so the mask is where image < threshold\nmascara_inicial = img_suavizada_seg < limiar_otsu_celula\n\n# Task 2.2: Morphological Cleaning (Cap. 9)\n# Remove small white objects (noise)\nmascara_limpa = morphology.remove_small_objects(mascara_inicial, 60) # Removes objects with fewer than 60 pixels\n\n# Fill holes inside main object\nmascara_final = ndimage.binary_fill_holes(mascara_limpa)\n\n# Step-by-step Segmentation Visualization\nplot_many_images(\n    [img_cinza_exemplo, mascara_inicial, mascara_limpa, mascara_final],\n    [\"Original Grayscale\", \"Initial Otsu Mask\", \"Clean Mask (noise removed)\", \"Final Mask (filled)\"],\n    rows=1, cols=4, cmaps='gray'\n)",
   "metadata": {
    "id": "bbBujg23h_IZ",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 453
    },
    "outputId": "df51cfa2-ab5b-44dc-8b8d-623d97b8baa7"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "**Interpreting the Results (Part 2):**\n\n- Initial Otsu Mask: Shows the first segmentation result. It may contain small holes or spurious objects.\n- Clean Mask: After removing small objects, the mask should contain less \"noise\".\n- Final Mask: The cell object should appear as a solid white blob, ready for feature extraction.\n\n\n**Exercise (Part 2):**\n\n- Thresholding (Ch. 10): Try using `filters.threshold_local` instead of `threshold_otsu`. Does segmentation improve or worsen for this image? Why?\n- Morphology (Ch. 9): In the `morphology.remove_small_objects` step, change `60` to `10` and then `200`. How does this affect the clean mask? What does this parameter control?\n",
   "metadata": {
    "id": "LyS7e2A_ZcbN"
   }
  },
  {
   "cell_type": "markdown",
   "source": "## Part 3: Feature Extraction\n**Brief Explanation:**\nNow that we isolated the cell with a mask, we can describe it quantitatively by extracting features (descriptors). We will extract shape, color, and texture features.\n\n**How the Code Works:**\n\n1. `measure.label` and `measure.regionprops`: We use the final mask to label object(s). `regionprops` then computes several properties for each labeled region.\n1. Shape Descriptors: We extract area, perimeter, eccentricity (how elongated the shape is), solidity (how \"solid\" vs. irregular the shape is), and Hu moments (a set of 7 values invariant to translation, scale, and rotation that describe shape).\n1. Color Descriptors: We use the mask to select cell pixels in the original RGB image and compute mean of each color channel (R, G, B).\n1. Texture Descriptors: We use the mask on the grayscale image to compute mean and standard deviation of cell pixel intensities.\n",
   "metadata": {
    "id": "oRfABJHbaCGy"
   }
  },
  {
   "cell_type": "code",
   "source": "# --- 3.1 Feature Extraction (Cap. 11) ---\n\n# Use mascara_final from previous step and original image (RGB and grayscale)\n\n# Label region in final mask for regionprops\nlabels_celula = measure.label(mascara_final)\n\n# Extract region properties (assuming only 1 main object)\nif labels_celula.max() == 0: # If segmentation fails and finds no object\n    print(\"No object found in segmentation. Skipping feature extraction.\")\n    features_dict = None\nelse:\n    propriedades = measure.regionprops(labels_celula, intensity_image=img_cinza_exemplo)\n    prop_obj_principal = propriedades[0] # Assume largest object is first/only\n\n    # --- Shape Descriptors ---\n    area = prop_obj_principal.area\n    perimetro = prop_obj_principal.perimeter\n    excentricidade = prop_obj_principal.eccentricity\n    solidez = prop_obj_principal.solidity\n\n    # Momentos de Hu (usando cv2)\n    # Image for moments must be uint8\n    mascara_ubyte_hu = img_as_ubyte(mascara_final)\n    momentos_espaciais = cv2.moments(mascara_ubyte_hu)\n    momentos_hu = cv2.HuMoments(momentos_espaciais).flatten()\n\n    # --- Color Descriptors ---\n    pixels_celula_rgb = img_rgb_exemplo[mascara_final]\n    media_R, media_G, media_B = np.mean(pixels_celula_rgb, axis=0)\n\n    # --- Texture Descriptors ---\n    pixels_celula_cinza = img_cinza_exemplo[mascara_final]\n    media_intensidade = np.mean(pixels_celula_cinza)\n    std_intensidade = np.std(pixels_celula_cinza)\n\n    # Compile all features into a dictionary\n    features_dict = {\n        'area': area,\n        'perimetro': perimetro,\n        'excentricidade': excentricidade,\n        'solidez': solidez,\n        'media_R': media_R,\n        'media_G': media_G,\n        'media_B': media_B,\n        'media_intensidade': media_intensidade,\n        'std_intensidade': std_intensidade,\n    }\n    # Add Hu moments\n    for i, hu_val in enumerate(momentos_hu):\n        features_dict[f'hu_{i}'] = hu_val\n\n    print(\"\\n--- Extracted Features from Segmented Cell ---\")\n    for nome, valor in features_dict.items():\n        print(f\"  {nome}: {valor:.4f}\")\n\n# Visualize segmentation overlay on original image\nsegmentacao_sobreposta = color.label2rgb(labels_celula, image=img_rgb_exemplo, bg_label=0, alpha=0.3)\nplot_many_images(\n    [img_rgb_exemplo, mascara_final, segmentacao_sobreposta],\n    [\"Original\", \"Final Mask\", \"Overlay Segmentation\"],\n    1, 3, cmaps=[None, 'gray', None], figsize=(15,5)\n)",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 818
    },
    "id": "a458kzS1WH4j",
    "outputId": "95b97aec-9e30-45f7-cfa3-9877dfa50dfc"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "**Interpreting the Results (Part 3):**\n\n- The printed output shows descriptor values for the example cell. These numbers are what the computer \"sees.\"\n- The \"Overlay Segmentation\" visualization helps confirm whether the final mask truly isolated the cell correctly.\n\n**Exercise (Part 3):**\n\n1. Descriptors (Ch. 11): Looking at dataset cell types (basophil, eosinophil, lymphocyte, etc.), which features (area, eccentricity, color, texture) do you think would be most useful for distinguishing a \"neutrophil\" from a \"lymphocyte\"? (This may require a quick review of cell morphology).\n1. The Fourier Transform is mentioned in Chapter 4. How could Fourier Descriptors (mentioned in Ch. 11) be used here? What would they describe? (Hint: they describe the cell contour in the frequency domain).\n",
   "metadata": {
    "id": "zEiI3Pm2rgJC"
   }
  },
  {
   "cell_type": "markdown",
   "source": "## Part 4: Object Classification\n**Brief Explanation:**\nNow we will use the features we learned to extract in order to train a machine learning classifier. The process will be:\n\n1. Process a subset of images, applying segmentation and feature extraction to each one.\n1. Split our feature dataset into train and test sets.\n1. Train a classifier (we will use a simple MLP neural network) on the training data.\n1. Evaluate classifier performance on test data.\n\n\n**How the Code Works:**\n\n1. Feature Extraction Loop:\n- Iterates over `num_amostras_total` images from the dataset.\n- For each image, applies the full segmentation and feature extraction pipeline we built.\n- Stores feature dictionary and corresponding label in a list.\n2. DataFrame Creation: List of dictionaries is converted into a Pandas DataFrame, a convenient tabular data structure.\n3. Training Preparation:\n- `X` contains features, `y` contains labels.\n- `train_test_split` divides `X` and `y` into train and test sets.\n- `StandardScaler` normalizes features. This is very important for good neural-network performance.\n4. Training and Evaluation:\n- `MLPClassifier(...)`: Defines neural network architecture.\n- `.fit(X_train, y_train)`: Trains classifier.\n- `.predict(X_test)`: Predicts on test set.\n- `accuracy_score` and `confusion_matrix`: Evaluate model performance.\n",
   "metadata": {
    "id": "6_N2ke_8rwlh"
   }
  },
  {
   "cell_type": "code",
   "source": "# --- 4.1 Object Classification (Cap. 12) ---\nprint(\"\\n--- Module 4: Object Classification ---\")\n\n# Function to encapsulate segmentation and extraction pipeline for one image\ndef extrair_features_de_imagem(img_rgb):\n    try:\n        # Preprocessing and Segmentation\n        img_cinza = rgb2gray(img_rgb)\n        img_suavizada = filters.gaussian(img_cinza, sigma=1)\n        limiar_otsu = filters.threshold_otsu(img_suavizada)\n        mascara_inicial = img_suavizada < limiar_otsu\n        mascara_limpa = morphology.remove_small_objects(mascara_inicial, 60)\n        mascara_final = ndimage.binary_fill_holes(mascara_limpa)\n\n        # Feature Extraction\n        labels = measure.label(mascara_final)\n        if labels.max() == 0: return None # Segmentation failed\n\n        # Find largest object if there is more than one\n        props = measure.regionprops(labels, intensity_image=img_cinza)\n        maior_obj_idx = np.argmax([p.area for p in props])\n        prop_obj_principal = props[maior_obj_idx]\n\n        # Descriptors\n        area = prop_obj_principal.area\n        perimetro = prop_obj_principal.perimeter\n        excentricidade = prop_obj_principal.eccentricity\n        solidez = prop_obj_principal.solidity\n\n        mascara_obj_principal = labels == (maior_obj_idx + 1)\n\n        pixels_celula_rgb = img_rgb[mascara_obj_principal]\n        media_R, media_G, media_B = np.mean(pixels_celula_rgb, axis=0) if pixels_celula_rgb.size > 0 else (0,0,0)\n\n        pixels_celula_cinza = img_cinza[mascara_obj_principal]\n        media_intensidade = np.mean(pixels_celula_cinza) if pixels_celula_cinza.size > 0 else 0\n        std_intensidade = np.std(pixels_celula_cinza) if pixels_celula_cinza.size > 0 else 0\n\n        momentos_espaciais = cv2.moments(img_as_ubyte(mascara_obj_principal))\n        momentos_hu = cv2.HuMoments(momentos_espaciais).flatten()\n\n        features_dict = {\n            'area': area, 'perimetro': perimetro, 'excentricidade': excentricidade,\n            'solidez': solidez, 'media_R': media_R, 'media_G': media_G,\n            'media_B': media_B, 'media_intensidade': media_intensidade, 'std_intensidade': std_intensidade,\n        }\n        for i, hu_val in enumerate(momentos_hu):\n            features_dict[f'hu_{i}'] = hu_val\n\n        return features_dict\n\n    except Exception as e:\n        # print(f\"Error processing an image: {e}\")\n        return None\n\n# Loop to process a subset of images (e.g., 500 for a quick test)\nnum_amostras_total = 1000 # Reduce if it is taking too long (ex: 500)\nall_features = []\nall_labels = []\n\nprint(f\"Extracting features from {num_amostras_total} images. This may take a minute...\")\nfor i in range(min(num_amostras_total, len(images))):\n    img_atual = images[i]\n    label_atual = labels[i]\n\n    features = extrair_features_de_imagem(img_atual)\n    if features is not None:\n        all_features.append(features)\n        all_labels.append(label_atual)\n\n# Create a Pandas DataFrame\ndf_features = pd.DataFrame(all_features)\ndf_features['label'] = all_labels\n\nprint(f\"Processing completed. {len(df_features)} cells were segmented and had their features extracted.\")\nprint(\"Feature DataFrame sample:\")\nprint(df_features.head())\n\n# --- Classifier Training and Evaluation ---\nif not df_features.empty:\n    X = df_features.drop('label', axis=1) # All columns except label\n    y = df_features['label']\n\n    # Split into train and test\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n\n    # Scale features\n    scaler = StandardScaler()\n    X_train_scaled = scaler.fit_transform(X_train)\n    X_test_scaled = scaler.transform(X_test)\n\n    # Train an MLP classifier (Neural Network)\n    print(\"\\nTraining Neural Network (MLPClassifier)...\")\n    mlp = MLPClassifier(hidden_layer_sizes=(50, 25), max_iter=500, random_state=42, early_stopping=True, n_iter_no_change=20)\n    mlp.fit(X_train_scaled, y_train)\n\n    # Evaluate on test set\n    y_pred = mlp.predict(X_test_scaled)\n    acuracia = accuracy_score(y_test, y_pred)\n    print(f\"MLP classifier accuracy on test set: {acuracia:.4f}\")\n\n    # Confusion Matrix\n    print(\"Generating Confusion Matrix...\")\n    nomes_classes_str = [class_names[str(i)] for i in sorted(y.unique())]\n    cm = confusion_matrix(y_test, y_pred)\n    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=nomes_classes_str)\n\n    fig, ax = plt.subplots(figsize=(10, 10))\n    disp.plot(ax=ax, xticks_rotation='vertical')\n    plt.title(\"Confusion Matrix\")\n    plt.show()\nelse:\n    print(\"No features were extracted, classification step was skipped.\")",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "B0JFCMCsWP4s",
    "outputId": "9be71522-1809-4c75-f8cc-fd9a6b1c5dd9"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "**Interpreting the Results (Part 4):**\n\n- DataFrame Sample: Shows a table with extracted features for the first cells.\n- Accuracy: A percentage indicating how many test-set cells were correctly classified. An accuracy of 0.85 means 85% correct.\n\n\nConfusion Matrix: A visual table showing classifier performance in detail.\n- Main diagonal (top-left to bottom-right) shows correct classifications.\n- Off-diagonal values are errors. For example, the cell at row \"basophil\" and column \"eosinophil\" indicates how many basophils were incorrectly classified as eosinophils.\n- Helps identify which classes are hardest for the model to distinguish.\n\n\n**Final Exercise and Discussion:**\n\n1. Accuracy and Confusion Matrix: Was the achieved accuracy good? Looking at the confusion matrix, which classes did the model struggle most to distinguish (largest off-diagonal values)?\n1. Improving the System: How could you improve this system’s performance? Discuss at least three ideas based on concepts from all chapters covered. (E.g., use more features (Ch. 11), use a more robust segmentation model (Ch. 10), use a different classifier or architecture (Ch. 12), use more training data, etc.).\n1. Ethical Implications: Discuss ethical implications, challenges, and responsibilities of building and deploying a computer-aided diagnostic system like this in a real clinical environment. What would happen if the system incorrectly classified a malignant cell as benign, or vice versa?\n",
   "metadata": {
    "id": "XIEHE4fJsUWJ"
   }
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {
    "id": "z9kVBTLEWfSt"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}